{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a76d2f",
   "metadata": {},
   "source": [
    "# ACT CW2 Q2\n",
    "\n",
    "__Q2 Objective:__\n",
    "\n",
    "Process dataset using a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c7515",
   "metadata": {},
   "source": [
    "__Plan__\n",
    "\n",
    "We have a binary classification problem, so we are sorting data into one of two classes based on the input values.\n",
    "\n",
    "\n",
    "__Workflow__ (from Source 1)\n",
    "\n",
    "1. Get data ready (transform into tensors)\n",
    "2. Build or pick a pretrained model to suit your problem\n",
    "3. Pick a loss function and optimiser\n",
    "4. Build a training loop\n",
    "(Loop through steps 2-4)\n",
    "5. Fit the model to the data and make a prediction\n",
    "6. Evaluate the model\n",
    "7. Improve through experimentation\n",
    "8. Save and reload the trained model\n",
    "\n",
    "\n",
    "__Links__ \n",
    "\n",
    "(move to the bottom in a bit)\n",
    "\n",
    "* https://www.learnpytorch.io/00_pytorch_fundamentals/\n",
    "* https://www.learnpytorch.io/01_pytorch_workflow/\n",
    "* https://www.learnpytorch.io/02_pytorch_classification/\n",
    "* https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "* \n",
    "\n",
    "\n",
    "More general links:\n",
    "\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html\n",
    "* https://www.geeksforgeeks.org/deep-learning/converting-a-pandas-dataframe-to-a-pytorch-tensor/\n",
    "* https://saturncloud.io/blog/how-do-i-convert-a-pandas-dataframe-to-a-pytorch-tensor/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "* https://www.geeksforgeeks.org/pandas/adding-new-column-to-existing-dataframe-in-pandas/\n",
    "* https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "* https://codesignal.com/learn/courses/introduction-to-pytorch-tensors/lessons/defining-a-dataset-with-pytorch-tensors\n",
    "* https://docs.pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "* https://discuss.pytorch.org/t/what-do-tensordataset-and-dataloader-do/107017\n",
    "* https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "* https://docs.pytorch.org/docs/stable/nn.html\n",
    "* https://realpython.com/ref/glossary/subclass/\n",
    "* https://www.w3schools.com/python/python_classes.asp\n",
    "* https://stackoverflow.com/questions/8609153/why-do-we-use-init-in-python-classes\n",
    "* https://stackoverflow.com/questions/2709821/what-is-the-purpose-of-the-self-parameter-why-is-it-needed\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "* https://www.geeksforgeeks.org/deep-learning/understanding-the-forward-function-output-in-pytorch/\n",
    "* https://docs.pytorch.org/docs/stable/nn.html\n",
    "* https://www.geeksforgeeks.org/python/activation-functions-in-pytorch/\n",
    "* https://machinelearningmastery.com/activation-functions-in-pytorch/\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html\n",
    "* https://stackoverflow.com/questions/75979632/pytorchs-nn-bcewithlogitsloss-behaves-totaly-differently-than-nn-bceloss\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "* https://docs.pytorch.org/docs/stable/generated/torch.nn.Dropout.html~\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bc624",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27c940e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import numpy as np # numpy\n",
    "import pandas as pd # for dataframes\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "# import seaborn as sns # for data visualisation\n",
    "\n",
    "# machine learning libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch # pytorch library\n",
    "from torch.utils.data import TensorDataset, DataLoader # for batching data\n",
    "from torch import nn, optim # neural networks and optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fcf6b",
   "metadata": {},
   "source": [
    "### Set up GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95c2493",
   "metadata": {},
   "source": [
    "If there is a GPU available for PyTorch to use, this will be much faster for running the neural network than using the CPU, as GPUs are much faster for executing matrix multiplication operations.\n",
    "\n",
    "__Note:__ \n",
    "\n",
    "My computer's processor has an integrated graphics card and no dedicated GPU. The code cell below will return False and any code run on my personal computer will be run on the CPU, which may be slower or less efficient. \n",
    "\n",
    "However, I will still include the code cell below for cases when this code is run on a PC or Google Colab. If using Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7b11b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available:\n",
      "False\n",
      "\n",
      "Device:\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# is there a GPU available?\n",
    "gpu_TF = torch.cuda.is_available()\n",
    "\n",
    "# print the result\n",
    "print(\"GPU available:\")\n",
    "print(gpu_TF)\n",
    "\n",
    "# if there is a GPU available\n",
    "\n",
    "if gpu_TF == True:\n",
    "    # there is a GPU available\n",
    "    # set the device to the graphics processor\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    # there is no GPU available\n",
    "    # use the CPU instead\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# print the device we are using\n",
    "print(\"\\nDevice:\")\n",
    "print(device)\n",
    "\n",
    "# we can now store tensors on the selected device using .to(device)\n",
    "# and this will work for both cpu and gpu devices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfb130",
   "metadata": {},
   "source": [
    "## \n",
    "> ## Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b84dc",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be4024",
   "metadata": {},
   "source": [
    "Load in the data file (currently in csv file), add the data to a pandas dataframe, and inspect the dataframe to check that it has loaded in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45d5ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data file\n",
    "# data is in the file \"psion_upsilon.csv\"\n",
    "\n",
    "# read in the file and store it in a pandas dataframe\n",
    "rawdata_df = pd.read_csv('psion_upsilon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51598101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe:\n",
      "(40000, 22)\n",
      "\n",
      "\n",
      "Column headings:\n",
      "Index(['Unnamed: 0', 'Run', 'Event', 'type1', 'E1', 'px1', 'py1', 'pz1', 'pt1',\n",
      "       'eta1', 'phi1', 'Q1', 'type2', 'E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2',\n",
      "       'phi2', 'Q2', 'class'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Top few rows of dataframe:\n",
      "<bound method NDFrame.head of        Unnamed: 0     Run       Event type1       E1      px1      py1  \\\n",
      "0               0  167807  1101779335     G   5.8830   3.6101   2.3476   \n",
      "1               1  167102   286049970     G  13.7492  -1.9921  11.8723   \n",
      "2               2  160957   190693726     G   8.5523   1.4623   4.5666   \n",
      "3               3  166033   518823971     G   7.5224   0.1682  -3.5854   \n",
      "4               4  163589    49913789     G  12.4683   8.1310  -1.6633   \n",
      "...           ...     ...         ...   ...      ...      ...      ...   \n",
      "39995       39995  166033   460063858     G  21.1411  -9.3928  10.8857   \n",
      "39996       39996  173692   573648364     G  29.4819  16.1461  21.9823   \n",
      "39997       39997  166895   139351693     G   9.2451  -4.9421   2.0173   \n",
      "39998       39998  165617   163336846     G  11.2984 -10.6532  -1.8375   \n",
      "39999       39999  165548   649908689     G  21.5936  11.3939 -12.8656   \n",
      "\n",
      "           pz1      pt1    eta1  ...  type2       E2      px2      py2  \\\n",
      "0       4.0069   4.3062  0.8314  ...      G   5.5776  -3.4452  -3.3512   \n",
      "1      -6.6416  12.0382 -0.5270  ...      G  11.9112  -3.9571   8.8601   \n",
      "2       7.0809   4.7950  1.1818  ...      G   6.3894  -0.1645  -4.6018   \n",
      "3       6.6100   3.5894  1.3704  ...      G   4.6434   0.5164  -3.9193   \n",
      "4      -9.3042   8.2994 -0.9644  ...      G   6.9360   4.9975  -4.8073   \n",
      "...        ...      ...     ...  ...    ...      ...      ...      ...   \n",
      "39995  15.4987  14.3779  0.9354  ...      G  26.7982 -13.6750  10.9520   \n",
      "39996  11.1918  27.2749  0.3996  ...      G  12.5656   5.6608   9.8999   \n",
      "39997  -7.5476   5.3380 -1.1461  ...      T   4.0363   3.9273  -0.7219   \n",
      "39998  -3.2825  10.8105 -0.2992  ...      G   7.9206  -7.9117   0.2587   \n",
      "39999 -13.0740  17.1855 -0.7017  ...      G   8.8155   4.8116  -6.0919   \n",
      "\n",
      "           pz2      pt2    eta2    phi2  Q2    class  \n",
      "0      -2.8283   4.8062 -0.5589 -2.3700   1  upsilon  \n",
      "1      -6.9069   9.7036 -0.6623  1.9908  -1    J/psi  \n",
      "2       4.4282   4.6048  0.8540 -1.6065  -1  upsilon  \n",
      "3       2.4336   3.9532  0.5822 -1.4398  -1    J/psi  \n",
      "4      -0.1063   6.9344 -0.0153 -0.7660  -1  upsilon  \n",
      "...        ...      ...     ...     ...  ..      ...  \n",
      "39995  20.2776  17.5201  0.9884  2.4663  -1    J/psi  \n",
      "39996   5.2754  11.4041  0.4475  1.0514  -1    J/psi  \n",
      "39997  -0.5796   3.9931 -0.1446 -0.1818  -1  upsilon  \n",
      "39998  -0.2494   7.9160 -0.0315  3.1089   1    J/psi  \n",
      "39999  -4.1759   7.7629 -0.5149 -0.9023  -1    J/psi  \n",
      "\n",
      "[40000 rows x 22 columns]>\n"
     ]
    }
   ],
   "source": [
    "# check the dataframe has loaded in correctly\n",
    "\n",
    "# print the shape of the dataframe\n",
    "print(\"Size of dataframe:\")\n",
    "print(rawdata_df.shape) # 40,000 rows x 22 columns\n",
    "print(f\"\\n\")\n",
    "\n",
    "# print the column headings\n",
    "print(\"Column headings:\")\n",
    "print(rawdata_df.columns)\n",
    "print(f\"\\n\")\n",
    "\n",
    "# check top few rows of data\n",
    "print(\"Top few rows of dataframe:\")\n",
    "print(rawdata_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01e4ea",
   "metadata": {},
   "source": [
    "### Look at proportion of Binary Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d91b03",
   "metadata": {},
   "source": [
    "The nature of this problem is determining the type of the outgoing meson based on the properties of the 2 incoming muons. There are 2 possible types - J/psi and Upsilon.\n",
    "\n",
    "We can see how many instances of each meson there are in our dataset of 40,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9f4b4d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "upsilon    20000\n",
      "J/psi      20000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "class\n",
      "upsilon    0.5\n",
      "J/psi      0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# how many occurences of J/psi and upsilon in 'class'\n",
    "\n",
    "# how many samples in each class\n",
    "print(rawdata_df['class'].value_counts())\n",
    "\n",
    "print(\"\\n\") # space between outputs\n",
    "\n",
    "# what proportion of the samples are in each class\n",
    "print(rawdata_df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d520e",
   "metadata": {},
   "source": [
    "This dataset has an exact 50/50 split between the classes. As this is a segment of a larger (>5TB) dataset, this is probably by design. This should make it easier to train the neural network correctly, as the data shows no bias towards either of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ca4ee",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Data Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f97429",
   "metadata": {},
   "source": [
    "The first 3 columns contain index, run number, and event number. These are parameters used when recording and storing the data points, but they are not physical properties and do not have any effect on the type of particle created. Therefore, they are irrelevant to determining output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12217484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of reduced dataframe:\n",
      "(40000, 19)\n",
      "\n",
      "\n",
      "Column headings:\n",
      "Index(['type1', 'E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1',\n",
      "       'type2', 'E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2',\n",
      "       'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# remove the first 3 columns\n",
    "# by defining a new dataframe\n",
    "# that only contains the relevant variables\n",
    "\n",
    "# drop columns 0, 1, and 2\n",
    "# so keep all rows, and columns 3-21\n",
    "# (df.iloc indices are start inclusive and end exclusive)\n",
    "reduced_df = rawdata_df.iloc[:, 3:]\n",
    "\n",
    "# check the properites of the new dataframe are what we want\n",
    "# print out the new shape and column headings\n",
    "\n",
    "print(\"Size of reduced dataframe:\")\n",
    "print(reduced_df.shape)\n",
    "# 40,000 rows x 19 columns\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(\"Column headings:\")\n",
    "print(reduced_df.columns)\n",
    "\n",
    "# this is what we expect\n",
    "# we have removed 'Unnamed (index)', 'Run', and 'Event'\n",
    "# and kept all 40,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daee5e",
   "metadata": {},
   "source": [
    "Now that we have removed the first 3 columns, we can look at the other variables.\n",
    "\n",
    "To build a neural network in PyTorch, the data should be stored in a tensor, which can only contain numerical values. We can check the type of all of the columns in the reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "79c7f918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type1     object\n",
      "E1       float64\n",
      "px1      float64\n",
      "py1      float64\n",
      "pz1      float64\n",
      "pt1      float64\n",
      "eta1     float64\n",
      "phi1     float64\n",
      "Q1         int64\n",
      "type2     object\n",
      "E2       float64\n",
      "px2      float64\n",
      "py2      float64\n",
      "pz2      float64\n",
      "pt2      float64\n",
      "eta2     float64\n",
      "phi2     float64\n",
      "Q2         int64\n",
      "class     object\n",
      "dtype: object\n",
      "\n",
      "Summary of column types:\n",
      "float64    14\n",
      "object      3\n",
      "int64       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the datatypes of each column in the dataframe\n",
    "get_types = reduced_df.dtypes\n",
    "\n",
    "# print the data types of each column\n",
    "print(get_types)\n",
    "# and how many of each type there are\n",
    "print(\"\\nSummary of column types:\")\n",
    "print(get_types.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7d100",
   "metadata": {},
   "source": [
    "Of the 19 columns in the reduced dataframe, 16 of them are numeric (14 float and 2 int). These values can all be converted to a single numeric type (e.g. float64) upon transformation into a tensor. However, there are also 3 'object'-type variables that are unable to be processed by the neural network.\n",
    "\n",
    "The first 2 non-numeric columns are 'type1' and 'type2', which tell us the types of the first and second muon respectively, whether they are a global muon (G), or a tracker muon (T). The other non-numeric variable is 'class', which tells us the type of the meson created by the collision, either J/psi or Upsilon.\n",
    "\n",
    "There are a few options for dealing with these variables:\n",
    "\n",
    "1) Label encoding or assigning category codes. These methods assign an integer value to every possible string value. This is a very efficient method of converting non-numeric variables but can easily be misinterpreted by the neural network. Some machine learning models, including neural networks, will treat integer-encoded data as numeric, and make assumptions that are not true for categorical data, leading to incorrect assumptions and correlations. This can be avoid by using embedding layers in the neural network.\n",
    "2) One-hot encoding. This avoids the pitfalls of integer-encoding, but increases the dimensionality of the problem. Certain methods of one-hot encoding also produce Boolean values instead of numeric ones, which also cannot be interpreted by the neural network.\n",
    "3) Deleting the non-numeric values. This avoids the problem of encoding the data, but can lead to important variables being neglected and the neural network not learning the correct connections. If we decide not to include a certain variable, we must have a good reason for doing so.\n",
    "\n",
    "In deciding which of these method to use, we should consider the following points:\n",
    "\n",
    "* 'type1' contains only 1 unique value. As we found in Q1, 'type1' contained only 'G' values, so 100% of these particles were global muons. This variable can be immediately ignored as there can be no correlation between 'type1' and 'class'.\n",
    "* Proportion of each value in 'type2'. This column contains both 'G' and 'T' values, although around 90% of the samples have a 'G' value. This is a relatively small portion of the samples, especially when the division of class among the samples is a 50/50 split.\n",
    "* Feature importance. In Q1, after training a decision tree, the relative importance of each feature was plotted. The importance of 'type2' was one of the lowest, at only 0.2%. We should weigh up the possibility of introducing extra parameters with how useful the result may or may not be to the resulting neural network.\n",
    "* The type of each ingoing muon ('G' or 'T') is not a physical property of the muon, or a reflection of the physics governing it. It is a parameter denoting how the muon was detected, either locally or globally. As this only refers to the equipment used to detect the particle, and not information about the particle itself, it should not have any effect.\n",
    "* 'class' is our target variable, and cannot be ignored or deleted, so it must be encoded using one of the above methods.\n",
    "* The issues with integer-encoding don't apply to target data. The neural network treats labels differently to input data, because it only uses them to compare prediction with reality, not to learn connections. This means the 'class' variable can be label encoded with no loss of accuracy.\n",
    "\n",
    "Based on the factors above, the most efficient choice for this dataset is to remove the 'type1' and 'type2' columns entirely, and to encode the 'class' data using integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6f3844c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape:\n",
      "(40000, 17)\n",
      "\n",
      "New column headings:\n",
      "Index(['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1', 'E2', 'px2',\n",
      "       'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2', 'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# remove 'type1' and 'type2' from the dataset\n",
    "features_df = reduced_df.drop(['type1', 'type2'], axis=1)\n",
    "\n",
    "# the resultant dataframe contains only the relevant physical features\n",
    "\n",
    "# check that these 2 columns have been removed\n",
    "print(\"New shape:\")\n",
    "print(features_df.shape) # 40000 x 17\n",
    "print(\"\\nNew column headings:\")\n",
    "print(features_df.columns) # type columns no longer present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3c14e",
   "metadata": {},
   "source": [
    "### Label encoding the target data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89a5c4",
   "metadata": {},
   "source": [
    "The target data needs to be converted to numeric data before being transformed into a tensor. Because this is just the label array, we can take the simple approach of mapping each category to an integer without the risk of training the neural network incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5bb035ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class  class_int\n",
      "0  upsilon          1\n",
      "1    J/psi          0\n",
      "2  upsilon          1\n",
      "3    J/psi          0\n",
      "4  upsilon          1\n",
      "5  upsilon          1\n",
      "6    J/psi          0\n",
      "7  upsilon          1\n",
      "\n",
      "Integer mapping:\n",
      "{0: 'J/psi', 1: 'upsilon'}\n"
     ]
    }
   ],
   "source": [
    "# assign an integer value to each class type\n",
    "\n",
    "# add these values to a new column, 'class_int'\n",
    "# so we don't lose the original data\n",
    "\n",
    "# convert strings to numerical values\n",
    "# use category codes to assign integers\n",
    "features_df['class_int'] = features_df['class'].astype('category').cat.codes\n",
    "\n",
    "# print out the first few rows of both columns\n",
    "# first 8 rows, last 2 columns\n",
    "print(features_df.iloc[:8, -2:])\n",
    "\n",
    "# save the mapping so we can access it later\n",
    "class_mapping = dict(enumerate(features_df['class'].astype('category').cat.categories))\n",
    "\n",
    "# print the mapping key\n",
    "print(\"\\nInteger mapping:\")\n",
    "print(class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25e391",
   "metadata": {},
   "source": [
    "### Create Features matrix and Target array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebe5b6",
   "metadata": {},
   "source": [
    "We now have a dataframe with 18 columns, including 16 features and 1 target variable (across 2 columns). All of these variables are numeric in type and relate to physical properties of the particles.\n",
    "\n",
    "Before transforming into tensors, we should group the dataset into 2 separate objects - a feature matrix (X) and a target array (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "973d664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_total:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(40000, 16)\n",
      "\n",
      "y_total\n",
      "<class 'pandas.core.series.Series'>\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# split into feature matrix and target array\n",
    "# X and y\n",
    "\n",
    "# the feature matrix contains the input data\n",
    "# drop the 'class' and 'class_int' columns from the dataframe\n",
    "X_total = features_df.drop(['class', 'class_int'], axis=1)\n",
    "\n",
    "# the target array is the information with which we want the data to be classified (the \"label\")\n",
    "# this data is the end column of the features dataframe\n",
    "y_total = features_df['class_int']\n",
    "\n",
    "# check type and size of both X and y\n",
    "\n",
    "print(\"X_total:\")\n",
    "print(type(X_total)) # pd dataframe\n",
    "print(X_total.shape) # (40000, 16)\n",
    "\n",
    "print(\"\\ny_total\")\n",
    "print(type(y_total)) # pd series\n",
    "print(y_total.shape) # (40000,)\n",
    "\n",
    "# the features matrix has been split into the correct X and y arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4453a7",
   "metadata": {},
   "source": [
    "## \n",
    "> ## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d2f1a",
   "metadata": {},
   "source": [
    "### Convert Pandas objects into NumPy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e661959",
   "metadata": {},
   "source": [
    "We will eventually need the dataset in the form of a PyTorch tensor for use in the neural network. For a pandas dataframe, this means converting to a NumPy array first and then a tensor (even if this is not explicitly coded, it is still processed this way by the program). \n",
    "\n",
    "However, we first need to scale and split the data, which cannot be done on tensors directly, so should perform these operations on the intermediate NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da1baf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X:\n",
      "<class 'numpy.ndarray'>\n",
      "(40000, 16)\n",
      "\n",
      "y\n",
      "<class 'numpy.ndarray'>\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# dataframe.values\n",
    "# gets just the data and no column names\n",
    "# may also increase the storage of each datatype\n",
    "# by setting all columns to same datatype (probably float64)\n",
    "\n",
    "# convert X_total and y_total to numpy arrays\n",
    "X = X_total.values\n",
    "y = y_total.values\n",
    "\n",
    "# check the properties of these arrays\n",
    "\n",
    "print(\"\\nX:\")\n",
    "print(type(X)) # numpy array\n",
    "print(X.shape) # (40000, 16)\n",
    "\n",
    "print(\"\\ny\")\n",
    "print(type(y)) # numpy array\n",
    "print(y.shape) # (40000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51af465",
   "metadata": {},
   "source": [
    "### Split into Test and Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0228ef8",
   "metadata": {},
   "source": [
    "We don't want to use all of our data points to train the model. We can split the full dataset into data used to train the model (training data) and data that we can use to analyse the effectiveness of the model once it has been trained (test data).\n",
    "\n",
    "As there are 40,000 samples, we have enough data to do a 75/25 split. I will therefore use 30,000 samples to train the model and the other 10,000 to test the model.\n",
    "\n",
    "Even though we have a very evenly split dataset (50/50 split between the 2 output classes), we can still use stratify to make sure that we have a good balance of classes in each subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0a43b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use train_test_split to split the data\n",
    "# allocate some of the data for training the neural network\n",
    "# and the rest for checking its accuracy\n",
    "\n",
    "# use 30,000 samples for training\n",
    "# so train_size = 0.75, test_size = 0.25\n",
    "\n",
    "# use a random state integer for reproducible random shuffling\n",
    "\n",
    "# inputs are the X and y numpy arrays\n",
    "# split the data into 4 separate objects\n",
    "# use stratify to preserve class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=71, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4719be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "(30000, 16)\n",
      "<class 'numpy.ndarray'>\n",
      "y_train:\n",
      "(30000,)\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "X_test:\n",
      "(10000, 16)\n",
      "<class 'numpy.ndarray'>\n",
      "y_test:\n",
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check the types and sizes of the outputs\n",
    "\n",
    "# training data\n",
    "# should be 30,000 randomly selected samples\n",
    "print(\"X_train:\")\n",
    "print(X_train.shape) # 30000 x 16\n",
    "print(type(X_train)) # np array\n",
    "print(\"y_train:\")\n",
    "print(y_train.shape) # 30000 (1D)\n",
    "print(type(y_train)) # np array\n",
    "\n",
    "print(f\"\\n\") # space between outputs\n",
    "\n",
    "# test data\n",
    "# should be the 10,000 remaining samples\n",
    "print(\"X_test:\")\n",
    "print(X_test.shape) # 10000 x 16\n",
    "print(type(X_test)) # np array\n",
    "print(\"y_test:\")\n",
    "print(y_test.shape) # 10000\n",
    "print(type(y_test)) # np array\n",
    "\n",
    "# the arrays have been split randomly as specified in the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071445b8",
   "metadata": {},
   "source": [
    "### Split into Validation data as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42f953",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4f91a",
   "metadata": {},
   "source": [
    "Neural networks perform better when dealing with inputs on the same scale, and most of the functions used in a neural network work best when dealing with data that is roughly Gaussian (mean = 1, variance = 1). As the features of this dataset are all on different scales, they should be standardised before using them to train a neural network.\n",
    "\n",
    "However, we only want to use the training data to fit the scaler, so that we don't indirectly use any information from the testing set to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "080a844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean (by feature):\n",
      "[ 1.37120108e+01 -1.03732320e-01 -1.71637590e-01 -4.15723233e-02\n",
      "  7.57993686e+00 -8.73109000e-03 -5.99268167e-02  6.58666667e-02\n",
      "  1.14661400e+01  3.87268600e-02  1.71115413e-01  9.12610767e-02\n",
      "  7.33469605e+00  4.11902000e-03  7.42526667e-02 -6.58666667e-02]\n",
      "\n",
      "Standard Deviation (by feature):\n",
      "[10.26548918  6.56887743  6.47730307 14.43055082  5.46271391  1.25031988\n",
      "  1.82965108  0.99782843  9.36487508  6.38112183  6.35297916 11.74920959\n",
      "  5.30913823  1.05181092  1.80715698  0.99782843]\n"
     ]
    }
   ],
   "source": [
    "# initialise an instance of the standard scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit to the training data only\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform both sets of X data\n",
    "# using the parameters from X_train\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test)\n",
    "\n",
    "# all of the X data has now been normalised\n",
    "\n",
    "# to view the mean and std of each feature\n",
    "print(\"Mean (by feature):\")\n",
    "print(scaler.mean_)\n",
    "print(\"\\nStandard Deviation (by feature):\")\n",
    "print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e453464",
   "metadata": {},
   "source": [
    "### Transform dataframes into PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cee3e",
   "metadata": {},
   "source": [
    "We now have 4 NumPy arrays, split and standardised. In order to use them in the neural network, they need to be PyTorch tensors.\n",
    "\n",
    "(After building the neural network, add code pushing tensors .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d198e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "float64    14\n",
      "int64       2\n",
      "Name: count, dtype: int64\n",
      "y:\n",
      "int8\n"
     ]
    }
   ],
   "source": [
    "# transform X_train, X_test, y_train, and y_test\n",
    "# all into torch tensors\n",
    "\n",
    "# first check that they only contain numeric dtypes\n",
    "print(\"X:\")\n",
    "print(X_total.dtypes.value_counts())\n",
    "print(\"y:\")\n",
    "print(y_total.dtypes)\n",
    "\n",
    "# all types int64 and float64, which are fine for tensor transformations\n",
    "# convert them all to floats upon transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4cfabaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([30000, 16])\n",
      "\n",
      "y tensor:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([30000, 1])\n"
     ]
    }
   ],
   "source": [
    "# transform X arrays to tensors\n",
    "X_train_ten = torch.from_numpy(X_train).float()\n",
    "X_test_ten = torch.from_numpy(X_test).float()\n",
    "\n",
    "# transform y arrays to tensors\n",
    "# need to use .unsqueeze\n",
    "# to change shape (n_samples,) to (n_samples, 1)\n",
    "y_train_ten = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "y_test_ten = torch.from_numpy(y_test).float().unsqueeze(1)\n",
    "\n",
    "# use .float()\n",
    "# to convert all numerical types to floats\n",
    "\n",
    "# check properties of these tensors\n",
    "# type and shape\n",
    "# just check the training data\n",
    "\n",
    "# features\n",
    "print(\"X tensor:\")\n",
    "print(type(X_train_ten)) # tensor\n",
    "print(X_train_ten.shape) # 30000 x 16\n",
    "# target\n",
    "print(\"\\ny tensor:\")\n",
    "print(type(y_train_ten)) # tensor\n",
    "print(y_train_ten.shape) # 30000 x 1\n",
    "\n",
    "# these are both torch.tensor objects\n",
    "# and have the correct shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ccdde4",
   "metadata": {},
   "source": [
    "### Store data together using TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1b774622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataset.TensorDataset'>\n"
     ]
    }
   ],
   "source": [
    "# store all training data in one tensor\n",
    "train_set = TensorDataset(X_train_ten, y_train_ten)\n",
    "\n",
    "# do the same for test data\n",
    "test_set = TensorDataset(X_test_ten, y_test_ten)\n",
    "\n",
    "# we cannot check the sizes of these objects directly\n",
    "# as they not tensors\n",
    "print(type(train_set))\n",
    "# they are an object within the class TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e083342e",
   "metadata": {},
   "source": [
    "### Split training data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1b192028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each dataset into batches\n",
    "# for feeding into the neural network\n",
    "# use a standard batch size of 64\n",
    "\n",
    "# split the training data\n",
    "# shuffle before batching\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "\n",
    "# do the same for the test data\n",
    "# this does not need to be shuffled\n",
    "test_loader = DataLoader(test_set, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1331",
   "metadata": {},
   "source": [
    "### Plot the Training Data  (maybe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253b5d5",
   "metadata": {},
   "source": [
    "As we have the pairplot from the Q1 notebook, which uses an identical dataset, leave this bit for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee074e0f",
   "metadata": {},
   "source": [
    "## \n",
    "> ## Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ffea28",
   "metadata": {},
   "source": [
    "### Overview of Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f4634e",
   "metadata": {},
   "source": [
    "__Useful PyTorch Functions__\n",
    "\n",
    "* nn.Parameter\n",
    "* model.state_dict()\n",
    "* torch.randn()\n",
    "* torch.inference_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e173db8",
   "metadata": {},
   "source": [
    "__Model Architecture and Parameters:__\n",
    "\n",
    "- Input features = 16\n",
    "- Output features = 1\n",
    "- Number of hidden layers = 2\n",
    "- Number of neurons per hidden layer = 32-64\n",
    "- Fully connected layers\n",
    "- Activation function = ReLU\n",
    "- Loss function = BCEWithLogitsLoss\n",
    "- Optimiser = Adam\n",
    "\n",
    "<br>\n",
    "\n",
    "- Number of training iterations = ??\n",
    "- Confusion matrix = sklearn function (find)\n",
    "- Classification report = sklearn function (find)\n",
    "\n",
    "<br>\n",
    "\n",
    "__Still to figure out:__\n",
    "- What is a feedforward network?\n",
    "- How many training iterations should we do?\n",
    "- What is a training epoch?\n",
    "- How do we shuffle the data between epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa71e8",
   "metadata": {},
   "source": [
    "### Build the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd528c",
   "metadata": {},
   "source": [
    "Start by defining a model class and constructing the NN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a40e3b4",
   "metadata": {},
   "source": [
    "__Why have we chosen this network architecture?__\n",
    "\n",
    "- The number of input and output features are not a deliberate choice, but a result of the shape of the features matrix and target array. Each sample in the dataset has 16 physical parameters, and 1 label.\n",
    "\n",
    "- Number of hidden layers = 2. For a binary classification, we will not benefit from a deep network like other problems would (e.g. image analysis, text classification, numerical prediction). Adding too many layers to this problem would likely not increase the accuracy of the solution, but would make it more prone to overfitting.\n",
    "\n",
    "- 64 neurons in layer 1 and 32 in layer 2.\n",
    "\n",
    "- Fully connected layers\n",
    "\n",
    "- ReLU activation function between layers.\n",
    "\n",
    "- No sigmoid function after the final layer. As this is a classification problem, the output should be a probability, with a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac25a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a new model class called ParticleClassifier\n",
    "# that inherits from the nn.Module class\n",
    "# and so gives it access to PyTorch functionalities\n",
    "\n",
    "class ParticleClassifier(nn.Module):\n",
    "\n",
    "    # initialise an instance of this class\n",
    "    def __init__(self):\n",
    "\n",
    "        # initialise the parent class\n",
    "        super().__init__()\n",
    "        \n",
    "        # create a stack of (4 for now) hidden layers\n",
    "        # each with (8 for now / 12 / 16) neurons\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            # list of linear layers in order\n",
    "            # separated by commas\n",
    "            # these will be stacked by nn.Sequential\n",
    "\n",
    "            # in_features for first layer = number of features in X_train\n",
    "            # out_features(layer_n) = in_features(layer_n+1)\n",
    "            # out_features for final layer = number of features in y_train\n",
    "\n",
    "            nn.Linear(16, 64), # layer 1\n",
    "            nn.ReLU(), # activation function\n",
    "\n",
    "            nn.Linear(64, 64), # layer 2\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(64, 1) # output layer\n",
    "        ) # end of sequence\n",
    "\n",
    "    # define a new instance of the model class\n",
    "    # for the forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # pass x through all the NN layers\n",
    "        out = self.layer_stack(x)\n",
    "        # return the output to the training loop\n",
    "        return out\n",
    "    \n",
    "# end of class construction block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb41cc",
   "metadata": {},
   "source": [
    "### Initialise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d257064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do i need to create a random seed for the initial parameters?\n",
    "#torch.manual_seed(int)\n",
    "\n",
    "# create an instance of the model\n",
    "# and pass it to device (gpu / cpu)\n",
    "#model_v0 = ParticleClassifier().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "95184ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticleClassifier(\n",
      "  (layer_stack): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "<generator object Module.parameters at 0x000001D1A4DD7840>\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the model\n",
    "pc_model = ParticleClassifier()\n",
    "\n",
    "# print the model\n",
    "print(pc_model)\n",
    "\n",
    "# just prints out the structure we defined above\n",
    "\n",
    "# print the model parameters\n",
    "print(pc_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b6662",
   "metadata": {},
   "source": [
    "### Create a Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2968f2",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "\n",
    "torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "This function is an improvment upon torch.nn.BCELoss(), where BCE = Binary Cross Entropy. BCE is used to calculate the loss of accuracy in binary classification problems.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Loss function = BCEWithLogitsLoss. As we have a classification problem, the output is a probability, so must be between 0 and 1. The outputs from layer 2 may not produce the desired ouputs, so it is typical to add a sigmoid function after the final layer. However, this loss function, BCEWithLogitsLoss, is an improved version of its predecessor, BCELoss, that adds a sigmoid function automatically before optimising. This function is recommended over using 2 separate functions as it compiles both mathematical operations into 1 and reduces error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "737782b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce4763",
   "metadata": {},
   "source": [
    "### Create an Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b7f7e",
   "metadata": {},
   "source": [
    "- Optimiser function:\n",
    "\n",
    "Adam optimiser - torch.optim.Adam()\n",
    "\n",
    "- Learning rate:\n",
    "\n",
    "Default setting is 0.001. Through trial and error this has proved to be the most effective learning rate for this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d4a28ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a learning rate\n",
    "# default is 1e-3 (0.001)\n",
    "l_rate = 0.0\n",
    "\n",
    "# create an optimiser\n",
    "# using the parameters from our model\n",
    "# and the learning rate we have chosen\n",
    "optimiser = optim.Adam(params=pc_model.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dddf98a",
   "metadata": {},
   "source": [
    "### Train the model using a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa2fdaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a class\n",
    "# a model instance\n",
    "# a loss function\n",
    "# and an optimiser\n",
    "\n",
    "# we can now build a training loop\n",
    "# by following the steps in 01. PyTorch Workflow Fundamentals\n",
    "# section: PyTorch training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bffa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to 'training mode'\n",
    "pc_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20dbffb",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f0ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model to 'evaluation mode'\n",
    "model_v0.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
