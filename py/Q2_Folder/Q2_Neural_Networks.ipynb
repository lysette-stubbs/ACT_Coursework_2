{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a76d2f",
   "metadata": {},
   "source": [
    "# ACT CW2 Q2\n",
    "\n",
    "__Q2 Objective:__\n",
    "\n",
    "Process dataset using a Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c7515",
   "metadata": {},
   "source": [
    "__Plan__\n",
    "\n",
    "We have a binary classification problem, so we are sorting data into one of two classes based on the input values.\n",
    "\n",
    "\n",
    "__Workflow__ (from Source 1)\n",
    "\n",
    "1. Get data ready (transform into tensors)\n",
    "2. Build or pick a pretrained model to suit your problem\n",
    "3. Pick a loss function and optimiser\n",
    "4. Build a training loop\n",
    "(Loop through steps 2-4)\n",
    "5. Fit the model to the data and make a prediction\n",
    "6. Evaluate the model\n",
    "7. Improve through experimentation\n",
    "8. Save and reload the trained model\n",
    "\n",
    "\n",
    "__Links__ \n",
    "\n",
    "(move to the bottom in a bit)\n",
    "\n",
    "* https://www.learnpytorch.io/02_pytorch_classification/\n",
    "* https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "* \n",
    "\n",
    "\n",
    "More general links:\n",
    "\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.values.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html\n",
    "* https://www.geeksforgeeks.org/deep-learning/converting-a-pandas-dataframe-to-a-pytorch-tensor/\n",
    "* https://saturncloud.io/blog/how-do-i-convert-a-pandas-dataframe-to-a-pytorch-tensor/\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html\n",
    "* https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "* https://www.geeksforgeeks.org/pandas/adding-new-column-to-existing-dataframe-in-pandas/\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bc624",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "27c940e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import numpy as np # numpy\n",
    "import pandas as pd # for dataframes\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "# import seaborn as sns # for data visualisation\n",
    "\n",
    "# machine learning libraries and models\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdfb130",
   "metadata": {},
   "source": [
    "## \n",
    "> ## Preparing the Dataset\n",
    "\n",
    "Load in the classification data and prepare it as a PyTorch tensor.\n",
    "\n",
    "To start with, we want our data set (all relevant features) to be contained in a pandas dataframe and ready to solve.\n",
    "\n",
    "We will then move onto importing it into a PyTorch tensor.\n",
    "\n",
    "Therefore, the first part of this notebook will be very similar to Q1.\n",
    "\n",
    "After this, convert to a PyTorch tensor and use sklearn to perform a reproducible train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b84dc",
   "metadata": {},
   "source": [
    "### Load in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be4024",
   "metadata": {},
   "source": [
    "Load in the data file (currently in csv file), add the data to a pandas dataframe, and inspect the dataframe to check that it has loaded in correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "45d5ca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data file\n",
    "# data is in the file \"psion_upsilon.csv\"\n",
    "\n",
    "# read in the file and store it in a pandas dataframe\n",
    "rawdata_df = pd.read_csv('psion_upsilon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "51598101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataframe:\n",
      "(40000, 22)\n",
      "\n",
      "\n",
      "Column headings:\n",
      "Index(['Unnamed: 0', 'Run', 'Event', 'type1', 'E1', 'px1', 'py1', 'pz1', 'pt1',\n",
      "       'eta1', 'phi1', 'Q1', 'type2', 'E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2',\n",
      "       'phi2', 'Q2', 'class'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "Top few rows of dataframe:\n",
      "<bound method NDFrame.head of        Unnamed: 0     Run       Event type1       E1      px1      py1  \\\n",
      "0               0  167807  1101779335     G   5.8830   3.6101   2.3476   \n",
      "1               1  167102   286049970     G  13.7492  -1.9921  11.8723   \n",
      "2               2  160957   190693726     G   8.5523   1.4623   4.5666   \n",
      "3               3  166033   518823971     G   7.5224   0.1682  -3.5854   \n",
      "4               4  163589    49913789     G  12.4683   8.1310  -1.6633   \n",
      "...           ...     ...         ...   ...      ...      ...      ...   \n",
      "39995       39995  166033   460063858     G  21.1411  -9.3928  10.8857   \n",
      "39996       39996  173692   573648364     G  29.4819  16.1461  21.9823   \n",
      "39997       39997  166895   139351693     G   9.2451  -4.9421   2.0173   \n",
      "39998       39998  165617   163336846     G  11.2984 -10.6532  -1.8375   \n",
      "39999       39999  165548   649908689     G  21.5936  11.3939 -12.8656   \n",
      "\n",
      "           pz1      pt1    eta1  ...  type2       E2      px2      py2  \\\n",
      "0       4.0069   4.3062  0.8314  ...      G   5.5776  -3.4452  -3.3512   \n",
      "1      -6.6416  12.0382 -0.5270  ...      G  11.9112  -3.9571   8.8601   \n",
      "2       7.0809   4.7950  1.1818  ...      G   6.3894  -0.1645  -4.6018   \n",
      "3       6.6100   3.5894  1.3704  ...      G   4.6434   0.5164  -3.9193   \n",
      "4      -9.3042   8.2994 -0.9644  ...      G   6.9360   4.9975  -4.8073   \n",
      "...        ...      ...     ...  ...    ...      ...      ...      ...   \n",
      "39995  15.4987  14.3779  0.9354  ...      G  26.7982 -13.6750  10.9520   \n",
      "39996  11.1918  27.2749  0.3996  ...      G  12.5656   5.6608   9.8999   \n",
      "39997  -7.5476   5.3380 -1.1461  ...      T   4.0363   3.9273  -0.7219   \n",
      "39998  -3.2825  10.8105 -0.2992  ...      G   7.9206  -7.9117   0.2587   \n",
      "39999 -13.0740  17.1855 -0.7017  ...      G   8.8155   4.8116  -6.0919   \n",
      "\n",
      "           pz2      pt2    eta2    phi2  Q2    class  \n",
      "0      -2.8283   4.8062 -0.5589 -2.3700   1  upsilon  \n",
      "1      -6.9069   9.7036 -0.6623  1.9908  -1    J/psi  \n",
      "2       4.4282   4.6048  0.8540 -1.6065  -1  upsilon  \n",
      "3       2.4336   3.9532  0.5822 -1.4398  -1    J/psi  \n",
      "4      -0.1063   6.9344 -0.0153 -0.7660  -1  upsilon  \n",
      "...        ...      ...     ...     ...  ..      ...  \n",
      "39995  20.2776  17.5201  0.9884  2.4663  -1    J/psi  \n",
      "39996   5.2754  11.4041  0.4475  1.0514  -1    J/psi  \n",
      "39997  -0.5796   3.9931 -0.1446 -0.1818  -1  upsilon  \n",
      "39998  -0.2494   7.9160 -0.0315  3.1089   1    J/psi  \n",
      "39999  -4.1759   7.7629 -0.5149 -0.9023  -1    J/psi  \n",
      "\n",
      "[40000 rows x 22 columns]>\n"
     ]
    }
   ],
   "source": [
    "# check the dataframe has loaded in correctly\n",
    "\n",
    "# print the shape of the dataframe\n",
    "print(\"Size of dataframe:\")\n",
    "print(rawdata_df.shape) # 40,000 rows x 22 columns\n",
    "print(f\"\\n\")\n",
    "\n",
    "# print the column headings\n",
    "print(\"Column headings:\")\n",
    "print(rawdata_df.columns)\n",
    "print(f\"\\n\")\n",
    "\n",
    "# check top few rows of data\n",
    "print(\"Top few rows of dataframe:\")\n",
    "print(rawdata_df.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ca4ee",
   "metadata": {},
   "source": [
    "### Remove Unnecessary Data Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f97429",
   "metadata": {},
   "source": [
    "The first 3 columns contain index, run number, and event number. These are parameters used when recording and storing the data points, but they are not physical properties and do not have any effect on the type of particle created. Therefore, they are irrelevant to determining output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "12217484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of reduced dataframe:\n",
      "(40000, 19)\n",
      "\n",
      "\n",
      "Column headings:\n",
      "Index(['type1', 'E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1',\n",
      "       'type2', 'E2', 'px2', 'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2',\n",
      "       'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# remove the first 3 columns\n",
    "# by defining a new dataframe\n",
    "# that only contains the relevant variables\n",
    "\n",
    "# drop columns 0, 1, and 2\n",
    "# so keep all rows, and columns 3-21\n",
    "# (df.iloc indices are start inclusive and end exclusive)\n",
    "reduced_df = rawdata_df.iloc[:, 3:]\n",
    "\n",
    "# check the properites of the new dataframe are what we want\n",
    "# print out the new shape and column headings\n",
    "\n",
    "print(\"Size of reduced dataframe:\")\n",
    "print(reduced_df.shape)\n",
    "# 40,000 rows x 19 columns\n",
    "\n",
    "print(f\"\\n\")\n",
    "print(\"Column headings:\")\n",
    "print(reduced_df.columns)\n",
    "\n",
    "# this is what we expect\n",
    "# we have removed 'Unnamed (index)', 'Run', and 'Event'\n",
    "# and kept all 40,000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97daee5e",
   "metadata": {},
   "source": [
    "Now that we have removed the first 3 columns, we can look at the other variables.\n",
    "\n",
    "To build a neural network in PyTorch, the data should be stored in a tensor, which can only contain numerical values. We can check the type of all of the columns in the reduced dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "79c7f918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type1     object\n",
      "E1       float64\n",
      "px1      float64\n",
      "py1      float64\n",
      "pz1      float64\n",
      "pt1      float64\n",
      "eta1     float64\n",
      "phi1     float64\n",
      "Q1         int64\n",
      "type2     object\n",
      "E2       float64\n",
      "px2      float64\n",
      "py2      float64\n",
      "pz2      float64\n",
      "pt2      float64\n",
      "eta2     float64\n",
      "phi2     float64\n",
      "Q2         int64\n",
      "class     object\n",
      "dtype: object\n",
      "\n",
      "Summary of column types:\n",
      "float64    14\n",
      "object      3\n",
      "int64       2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# get the datatypes of each column in the dataframe\n",
    "get_types = reduced_df.dtypes\n",
    "\n",
    "# print the data types of each column\n",
    "print(get_types)\n",
    "# and how many of each type there are\n",
    "print(\"\\nSummary of column types:\")\n",
    "print(get_types.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b7d100",
   "metadata": {},
   "source": [
    "Of the 19 columns in the reduced dataframe, 16 of them are numeric (14 float and 2 int). These values can all be converted to a single numeric type (e.g. float64) upon transformation into a tensor. However, there are also 3 'object'-type variables that are unable to be processed by the neural network.\n",
    "\n",
    "The first 2 non-numeric columns are 'type1' and 'type2', which tell us the types of the first and second muon respectively, whether they are a global muon (G), or a tracker muon (T). The other non-numeric variable is 'class', which tells us the type of the meson created by the collision, either J/psi or Upsilon.\n",
    "\n",
    "There are a few options for dealing with these variables:\n",
    "\n",
    "1) Label encoding or assigning category codes. These methods assign an integer value to every possible string value. This is a very efficient method of converting non-numeric variables but can easily be misinterpreted by the neural network. Some machine learning models, including neural networks, will treat integer-encoded data as numeric, and make assumptions that are not true for categorical data, leading to incorrect assumptions and correlations. This can be avoid by using embedding layers in the neural network.\n",
    "2) One-hot encoding. This avoids the pitfalls of integer-encoding, but increases the dimensionality of the problem. Certain methods of one-hot encoding also produce Boolean values instead of numeric ones, which also cannot be interpreted by the neural network.\n",
    "3) Deleting the non-numeric values. This avoids the problem of encoding the data, but can lead to important variables being neglected and the neural network not learning the correct connections. If we decide not to include a certain variable, we must have a good reason for doing so.\n",
    "\n",
    "In deciding which of these method to use, we should consider the following points:\n",
    "\n",
    "* 'type1' contains only 1 unique value. As we found in Q1, 'type1' contained only 'G' values, so 100% of these particles were global muons. This variable can be immediately ignored as there can be no correlation between 'type1' and 'class'.\n",
    "* Proportion of each value in 'type2'. This column contains both 'G' and 'T' values, although around 90% of the samples have a 'G' value. This is a relatively small portion of the samples, especially when the division of class among the samples is a 50/50 split.\n",
    "* Feature importance. In Q1, after training a decision tree, the relative importance of each feature was plotted. The importance of 'type2' was one of the lowest, at only 0.2%. We should weigh up the possibility of introducing extra parameters with how useful the result may or may not be to the resulting neural network.\n",
    "* The type of each ingoing muon ('G' or 'T') is not a physical property of the muon, or a reflection of the physics governing it. It is a parameter denoting how the muon was detected, either locally or globally. As this only refers to the equipment used to detect the particle, and not information about the particle itself, it should not have any effect.\n",
    "* 'class' is our target variable, and cannot be ignored or deleted, so it must be encoded using one of the above methods.\n",
    "* The issues with integer-encoding don't apply to target data. The neural network treats labels differently to input data, because it only uses them to compare prediction with reality, not to learn connections. This means the 'class' variable can be label encoded with no loss of accuracy.\n",
    "\n",
    "Based on the factors above, the most efficient choice for this dataset is to remove the 'type1' and 'type2' columns entirely, and to encode the 'class' data using integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6f3844c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape:\n",
      "(40000, 17)\n",
      "\n",
      "New column headings:\n",
      "Index(['E1', 'px1', 'py1', 'pz1', 'pt1', 'eta1', 'phi1', 'Q1', 'E2', 'px2',\n",
      "       'py2', 'pz2', 'pt2', 'eta2', 'phi2', 'Q2', 'class'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# remove 'type1' and 'type2' from the dataset\n",
    "features_df = reduced_df.drop(['type1', 'type2'], axis=1)\n",
    "\n",
    "# the resultant dataframe contains only the relevant physical features\n",
    "\n",
    "# check that these 2 columns have been removed\n",
    "print(\"New shape:\")\n",
    "print(features_df.shape) # 40000 x 17\n",
    "print(\"\\nNew column headings:\")\n",
    "print(features_df.columns) # type columns no longer present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af3c14e",
   "metadata": {},
   "source": [
    "### Label encoding the target data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89a5c4",
   "metadata": {},
   "source": [
    "The target data needs to be converted to numeric data before being transformed into a tensor. Because this is just the label array, we can take the simple approach of mapping each category to an integer without the risk of training the neural network incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5bb035ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class  class_int\n",
      "0  upsilon          1\n",
      "1    J/psi          0\n",
      "2  upsilon          1\n",
      "3    J/psi          0\n",
      "4  upsilon          1\n",
      "5  upsilon          1\n",
      "6    J/psi          0\n",
      "7  upsilon          1\n",
      "\n",
      "Integer mapping:\n",
      "{0: 'J/psi', 1: 'upsilon'}\n"
     ]
    }
   ],
   "source": [
    "# assign an integer value to each class type\n",
    "\n",
    "# add these values to a new column, 'class_int'\n",
    "# so we don't lose the original data\n",
    "\n",
    "# convert strings to numerical values\n",
    "# use category codes to assign integers\n",
    "features_df['class_int'] = features_df['class'].astype('category').cat.codes\n",
    "\n",
    "# print out the first few rows of both columns\n",
    "# first 8 rows, last 2 columns\n",
    "print(features_df.iloc[:8, -2:])\n",
    "\n",
    "# save the mapping so we can access it later\n",
    "class_mapping = dict(enumerate(features_df['class'].astype('category').cat.categories))\n",
    "\n",
    "# print the mapping key\n",
    "print(\"\\nInteger mapping:\")\n",
    "print(class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25e391",
   "metadata": {},
   "source": [
    "### Create Features matrix and Target array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebe5b6",
   "metadata": {},
   "source": [
    "We now have a dataframe with 18 columns, including 16 features and 1 target variable (across 2 columns). All of these variables are numeric in type and relate to physical properties of the particles.\n",
    "\n",
    "Before transforming into tensors, we should group the dataset into 2 separate objects - a feature matrix (X) and a target array (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "973d664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(40000, 16)\n",
      "\n",
      "y\n",
      "<class 'pandas.core.series.Series'>\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# split into feature matrix and target array\n",
    "# X and y\n",
    "\n",
    "# the feature matrix contains the input data\n",
    "# drop the 'class' and 'class_int' columns from the dataframe\n",
    "X_total = features_df.drop(['class', 'class_int'], axis=1)\n",
    "\n",
    "# the target array is the information with which we want the data to be classified (the \"label\")\n",
    "# this data is the end column of the features dataframe\n",
    "y_total = features_df['class_int']\n",
    "\n",
    "# check type and size of both X and y\n",
    "\n",
    "print(\"X:\")\n",
    "print(type(X_total)) # pd dataframe\n",
    "print(X_total.shape) # (40000, 16)\n",
    "\n",
    "print(\"\\ny\")\n",
    "print(type(y_total)) # pd series\n",
    "print(y_total.shape) # (40000,)\n",
    "\n",
    "# the features matrix has been split into the correct X and y arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e453464",
   "metadata": {},
   "source": [
    "### Transform dataframes into PyTorch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769cee3e",
   "metadata": {},
   "source": [
    "(This bit needs editing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0c117f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X.values:\n",
      "<class 'numpy.ndarray'>\n",
      "(40000, 16)\n",
      "\n",
      "y.values\n",
      "<class 'numpy.ndarray'>\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# pip installed and imported torch\n",
    "# from X_total (dataframe) and y_total (series) to X_ten and y_ten (tensors)\n",
    "\n",
    "# we already have the types and sizes of X and y\n",
    "# we don't need to print them out again\n",
    "\n",
    "# dataframe.values\n",
    "# gets just the data and no column names\n",
    "# may also increase the storage of each datatype\n",
    "# by setting all columns to same datatype (probably float64)\n",
    "\n",
    "# X_total is a dataframe\n",
    "# X_total.values extracts only the data and neglects the headings\n",
    "# df.values is of type numpy array\n",
    "# but the shape of the array is unchanged\n",
    "\n",
    "print(\"\\nX.values:\")\n",
    "print(type(X_total.values)) # numpy array\n",
    "print(X_total.values.shape) # (40000, 17)\n",
    "\n",
    "print(\"\\ny.values\")\n",
    "print(type(y_total.values)) # numpy array\n",
    "print(y_total.values.shape) # (40000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4d198e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:\n",
      "float64    14\n",
      "int64       2\n",
      "Name: count, dtype: int64\n",
      "y:\n",
      "int8\n"
     ]
    }
   ],
   "source": [
    "# transform X and y to tensors\n",
    "# X_ten and y_ten\n",
    "\n",
    "# first check that they only contain numeric dtypes\n",
    "print(\"X:\")\n",
    "print(X_total.dtypes.value_counts())\n",
    "print(\"y:\")\n",
    "print(y_total.dtypes)\n",
    "\n",
    "# all types int64 and float64, which are fine for tensor transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfabaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([40000, 16])\n",
      "\n",
      "y tensor:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([40000])\n"
     ]
    }
   ],
   "source": [
    "# transform X_total to a tensor\n",
    "X_ten = torch.from_numpy(X_total.values).float()\n",
    "\n",
    "# now transform y_total to a tensor\n",
    "y_ten = torch.from_numpy(y_total.values).float()\n",
    "\n",
    "# ints will be automatically changed to floats\n",
    "# or can we specify this, just to be clearer?\n",
    "\n",
    "# check properties of these tensors\n",
    "# type and shape\n",
    "\n",
    "# features\n",
    "print(\"X tensor:\")\n",
    "print(type(X_ten)) # tensor\n",
    "print(X_ten.shape) # 40000 x 16\n",
    "# target\n",
    "print(\"\\ny tensor:\")\n",
    "print(type(y_ten)) # tensor\n",
    "print(y_ten.shape) # 40000 (1D)\n",
    "\n",
    "# these are both torch.tensor objects\n",
    "# and have the correct shape\n",
    "\n",
    "# we may need to use unsqueeze() to add a dimension to y_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2f5377",
   "metadata": {},
   "source": [
    "### Split into Test and Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee822a",
   "metadata": {},
   "source": [
    "We don't want to use all of our data points to train the model. We can split the full dataset into data used to train the model (training data) and data that we can use to analyse the effectiveness of the model once it has been trained (test data).\n",
    "\n",
    "As there are 40,000 samples, we have enough data to do a 50/50 split. I will therefore use 20,000 samples to train the model and the other 20,000 to test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d343651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "torch.Size([20000, 16])\n",
      "<class 'torch.Tensor'>\n",
      "y_train:\n",
      "torch.Size([20000])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "\n",
      "X_test:\n",
      "torch.Size([20000, 16])\n",
      "<class 'torch.Tensor'>\n",
      "y_test:\n",
      "torch.Size([20000])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# use train_test_split to split the data\n",
    "# allocate some of the data for training the neural network\n",
    "# and the rest for checking its accuracy\n",
    "\n",
    "# use 20,000 samples for training\n",
    "# so train_size = 0.5\n",
    "\n",
    "# use a random state integer for reproducible random shuffling\n",
    "\n",
    "# inputs are the X and y torch tensors\n",
    "# split the data into 4 separate objects\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ten, y_ten, train_size=0.5, random_state=71)\n",
    "\n",
    "# check the types and sizes of the outputs\n",
    "\n",
    "# training data\n",
    "# should be 20,000 randomly selected samples\n",
    "print(\"X_train:\")\n",
    "print(X_train.shape) # 20000 x 16\n",
    "print(type(X_train)) # tensor\n",
    "print(\"y_train:\")\n",
    "print(y_train.shape) # 20000 (1D)\n",
    "print(type(y_train)) # tensor\n",
    "\n",
    "print(f\"\\n\") # space between outputs\n",
    "\n",
    "# test data\n",
    "# should be the 20,000 remaining samples\n",
    "print(\"X_test:\")\n",
    "print(X_test.shape) # 20000 x 16\n",
    "print(type(X_test)) # tensor\n",
    "print(\"y_test:\")\n",
    "print(y_test.shape) # 20000\n",
    "print(type(y_test)) # tensor\n",
    "\n",
    "# the arrays have been split randomly as specified in the function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1331",
   "metadata": {},
   "source": [
    "### Plot the Training Data (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a40d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee074e0f",
   "metadata": {},
   "source": [
    "## \n",
    "> ## Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4756e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
